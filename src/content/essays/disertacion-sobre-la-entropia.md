Llevo toda mi vida investigando sobre cómo funciona el universo, y cuanto más investigo al respecto más cuenta me doy de lo poco que lo conozco. Cuando comencé la carrera de físicas estaba seguro de cómo funcionaba a rasgos generales el universo, que tenía información precisa y detallada de cuales eran las leyes del universo. A día de hoy, siendo físico desde hace más de ocho años, y aún cuando no ejerzo como tal, sigo cada año investigando por mi cuenta; sigo sin saber que demonios es un electrón.

No obstante, a pesar de mi supina ignorancia con respecto al universo, uno de los conceptos que más me está fascinando investigar durante este último año, es el de la entropía. Yo sin ser para nada un experto, tengo una vaga idea de que es y que significa que me gustaría compartir. Más allá de la magnitud termodinámica y sus expresión proporcionada por _Boltzmann_, me interesa entender mejor lo que deriva de ella para explicar el mundo que nos rodea.

Podemos entender la entropía de un sistema de distintas formas. La más común y como la inmensa mayoría de profesores de secundaria, bachillerato y físicos recién graduados la explican; es como el desorden de un sistema. El desorden, lo dices, y te quedas tranquilo, tal cual. ¿Qué narices significa que la entropía es el desorden de un sistema?, ¿acaso el sistema tiene una madre que le manda recoger la habitación y así se disminuye?. Esa afirmación refiriéndose al desorden, creo que es cierta, y es una muy buena entrada para empezar a investigar la entropía, pero se queda corta.

Una forma más amplia de entender la entropía es como la cantidad de estados posibles que acepta un sistema físico. Imagina una _flecha_, la flecha solo puede estar mirando hacia arriba o hacia abajo. Diríamos que una sola flecha tiene solo dos posibles estados. Esto implica que su entropía física medida en $J/K$ es, aplicando Boltzmann, $S = k_{B}\,N\ln\bigl(\Omega\bigr)$, donde $\Omega$ es el número de estados y $N$ el número total de flechas que tenemos en el sistema.

Con esto podemos ver que la entropía aumenta a más _flechas_ tengamos de forma lineal, ya que el término $N$ múltiplica, o a más posibles orientaciones tengan las flechas de forma logarítmica, ya que el término $\Omega$ multiplica dentro de un logaritmo.

Es decir, una entropía alta lo que nos indica es que nuestro sistema tiene muchas posibles configuraciones disponibles, y una entropía baja que nuestro sistema tiene muy pocas configuraciones disponibles. Dicho de otra forma, en un sistema de entropía alta, con muchas flechas y muchas orientaciones es muy difícil por ejemplo que todas las flechas estén de repente en un estado concreto, por ejemplo colocadas en la misma dirección, mientras que a más baja es la entropía más posibilidades hay de que un sistema tenga un estado concreto.

En todo momento, estamos hablando de posibilidades de estar en una configuración dada. ¿Qué significa esto? Cuando en el colegio nos decía que un gas ocupa todo el espacio disponible en un volumen, siempre se hablaba de que las partículas o moléculas tendían a alejarse lo máximo posible. La entropía en ese sistema era alta, porque un gas suele tener temperatura alta relativa, y temperatura alta en un átomo o molécula se traduce en más posibles configuraciones válidas para esa entidad. Entonces, lo que esperaríamos ver es que todas las partículas o moléculas en estado gaseoso en el volumen que manejamos estén separadas de forma aleatoria, y estén expandidas, y no esperamos verlas en estado sólido, con todas las partículas juntas en un punto localizado de ese volumen.

La cosa es, y aquí es donde viene la magia, que no esperamos verlo así, pero si que es posible, ya que existe la posibilidad física real, que en ese espacio finito en el que tenemos, por ejemplo, miles de millones de partículas de hidrógeno, una configuración válida del sistema es que todas se junten espontáneamente para formar un sólido de hidrógeno. No obstante es tan improbable que es posible que en toda la historia del universo no veríamos ese estado emerger.

Hemos dicho que a mayor entropía, mayor cantidad de estados disponibles de un sistema, y que el calor provoca que sean posibles más estados en un átomo, ya que sus configuraciones electrónicas posibles aumentan al ser más energético. Si vieras un video de una vela consumiéndose, sabrías identificar si va hacia delante o en retroceso, ya que verías si la vela se está consumiendo, o si está formándose desde su cera líquida. Pasar de estado sólido a estado líquido dada una cierta temperatura en la que el estado líquido es posible, y por tanto una entropía más alta, es más probable que el proceso inverso bajo las mismas condiciones.

No obstante, como ya hemos mencionado es simplemente estadística, y esto implica que aunque es más probable no es imposible. Esto de por sí ya es bastante sorprendente, lo que sin lugar a dudas para mí es más sorprendente, es que podemos afirmar que el mundo que percibimos está directamente relacionado con esto. Ya que lo que nosotros entendemos como tiempo, desde un punto de vista sensorial observable, y no como cantidad física, es el efecto directo de que la entropía siempre aumenta o se mantiene constante en un sistema. Es decir, derivamos de forma directa que el tiempo va hacia delante, del hecho de que la entropía siempre tiende a aumentar.

Por no alargarme más con esta pequeña disertación, por mucho que me gustaría, imagina que nos encontramos al final de los tiempos, en una hipotética muerte térmica del universo, en la que la entropía global es máxima. En ese punto de la _'historia'_, seríamos incapaces de decir hacia donde va el tiempo, si hacia delante o hacia atrás, ya que todos los estados posibles serían exactamente igual de probables.
